The large language model I chose to use is "Qwen3-0.6B", the 8-bit "Q8_0" quantization in particular. I chose this model because it's fairly small (639MB) but still performant. I found the 8-bit "Q8_0" model was faster than the 4-bit "Q4_K_M" model from Unsloth in the testing I did, and the KV cache is stored in "Q8_0" so I went with that quantized model. Again, most of the processing time comes from prefill, so it may take a little bit for the chatbot to begin generating its response.