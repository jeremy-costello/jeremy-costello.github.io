The embedding model I chose to use is "nomic-embed-text-v1.5". I chose not to use nomic's v2 model, as v1.5 is way smaller and performance in English isn't much different. For the v1.5 model, I used the 4-bit "Q4_K_M" quantization. This was mainly for the smaller download size, as I didn't find that speed changed a huge amount between this and the 8-bit "Q8_0" quantization. Most of the processing time comes from prefill anyways, so generating the document embeddings was pretty slow, but most in-browser uses of this model have few tokens so processing time is pretty quick.